{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7523a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import paramiko\n",
    "from datetime import date\n",
    "\n",
    "# Configure basic loggig\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define constants and file paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "ENV_FILE_PATH = os.path.join(PROJECT_ROOT, '.env')\n",
    "ANALYSIS_TRACKER_FILENAME = 'log_analysis_tracker.xlsx'\n",
    "ANALYSIS_TRACKER_PATH = os.path.join(DATA_DIR, ANALYSIS_TRACKER_FILENAME)\n",
    "\n",
    "\n",
    "# Column name in Excel containing remote log directory paths\n",
    "LOG_PATH_COLUMN = 'remote_log_directory'\n",
    "\n",
    "# Define SFTP Port for Docker container connection\n",
    "SFTP_PORT = 2222\n",
    "\n",
    "# Base dir for local logs\n",
    "LOCAL_LOG_STORAGE_BASE = os.path.join(DATA_DIR, 'downloaded_logs')\n",
    "\n",
    "# Keywords to search for in logs (Make these match your sample logs)\n",
    "SUCCESS_KEYWORD = 'Execution Return Code: 0'\n",
    "FAILURE_KEYWORD = '*** Failure'\n",
    "ERROR_KEYWORD = '*** Error:'\n",
    "\n",
    "# Column prefix for daily results in Excel\n",
    "RESULTS_COLUMN_PREFIX = 'analysis_results_'\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_analysis_tracker(filename: str, required_col: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads the analysis tracker Excel file into a pandas DataFrame.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        logging.error(f\"Tracker file not found: '{filename}'. Please create it.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Loading tracker file: '{filename}'\")\n",
    "    try:\n",
    "        df = pd.read_excel(filename, header=0)\n",
    "        logging.info(f\"Successfully loaded tracker with shape: {df.shape}\")\n",
    "        if required_col not in df.columns:\n",
    "            logging.error(f\"Tracker file '{filename}' is missing the required column: '{required_col}'\")\n",
    "            return None\n",
    "        logging.info(f\"Required column '{required_col}' found in tracker.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read tracker file '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_current_date_string() -> str:\n",
    "    \"\"\"Returns today's date as a string in 'YYYYMMDD' format.\"\"\"\n",
    "    return date.today().strftime('%Y%m%d')\n",
    "\n",
    "def get_log_download_directory(base_dir: str, date_string: str) -> str:\n",
    "    \"\"\"Constructs the path to the directory holding logs for a specific date.\"\"\"\n",
    "    return os.path.join(base_dir, f\"{date_string}_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db085d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 17:01:46,922 - INFO - Executing analysis function for directory: /Users/benkaan/Desktop/projects/remote-log-analysis-automation/data/downloaded_logs/20250501_logs\n",
      "2025-05-01 17:01:46,926 - INFO - Loading tracker file: '/Users/benkaan/Desktop/projects/remote-log-analysis-automation/data/log_analysis_tracker.xlsx'\n",
      "2025-05-01 17:01:47,000 - INFO - Successfully loaded tracker with shape: (12, 6)\n",
      "2025-05-01 17:01:47,001 - INFO - Required column 'remote_log_directory' found in tracker.\n",
      "2025-05-01 17:01:47,001 - INFO - Updating tracker DataFrame with results in column: analysis_results_20250501\n",
      "2025-05-01 17:01:47,002 - INFO - Added new results column: analysis_results_20250501\n",
      "2025-05-01 17:01:47,002 - INFO - Matched 'job_conversion_rate-20250501_083000.log' (status: error) to 1 tracker rows (Indices: [4]).\n",
      "2025-05-01 17:01:47,003 - INFO - Matched 'job_payment_proc-20250501_083000.log' (status: error) to 1 tracker rows (Indices: [9]).\n",
      "2025-05-01 17:01:47,004 - INFO - Matched 'job_report_monthly-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [8]).\n",
      "2025-05-01 17:01:47,004 - INFO - Matched 'job_tax_calc-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [10]).\n",
      "2025-05-01 17:01:47,005 - INFO - Matched 'job_email_blast-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [2]).\n",
      "2025-05-01 17:01:47,006 - INFO - Matched 'job_update_crm-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [1]).\n",
      "2025-05-01 17:01:47,006 - INFO - Matched 'job_web_traffic-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [3]).\n",
      "2025-05-01 17:01:47,007 - INFO - Matched 'job_report_daily-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [6]).\n",
      "2025-05-01 17:01:47,007 - INFO - Matched 'job_roi_report-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [5]).\n",
      "2025-05-01 17:01:47,008 - INFO - Matched 'job_invoice_gen-20250501_083000.log' (status: success) to 1 tracker rows (Indices: [11]).\n",
      "2025-05-01 17:01:47,008 - INFO - Matched 'job_report_weekly-20250501_083000.log' (status: error) to 1 tracker rows (Indices: [7]).\n",
      "2025-05-01 17:01:47,009 - INFO - Matched 'job_segment_users-20250501_083000.log' (status: error) to 1 tracker rows (Indices: [0]).\n",
      "2025-05-01 17:01:47,009 - INFO - Performing final status updates for rows without direct log matches...\n",
      "2025-05-01 17:01:47,010 - INFO - Tracker update process finished.\n",
      "2025-05-01 17:01:47,010 - INFO - Attempting to save updated tracker to: /Users/benkaan/Desktop/projects/remote-log-analysis-automation/data/log_analysis_tracker.xlsx\n",
      "2025-05-01 17:01:47,019 - INFO - Analysis results saved successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Updated Tracker DataFrame Head ---\n",
      "                            remote_log_directory analysis_results_20250501  \\\n",
      "0    /logs/marketing/campaigns/job_segment_users                     error   \n",
      "1       /logs/marketing/campaigns/job_update_crm                   success   \n",
      "2      /logs/marketing/campaigns/job_email_blast                   success   \n",
      "3      /logs/marketing/analytics/job_web_traffic                   success   \n",
      "4  /logs/marketing/analytics/job_conversion_rate                     error   \n",
      "\n",
      "  analysis_results_20250430 analysis_results_20250429    project department  \\\n",
      "0                     error                     error  marketing  campaigns   \n",
      "1                   success                   success  marketing  campaigns   \n",
      "2                   success                   success  marketing  campaigns   \n",
      "3                   success                   success  marketing  analytics   \n",
      "4                     error                     error  marketing  analytics   \n",
      "\n",
      "              job_name  \n",
      "0    job_segment_users  \n",
      "1       job_update_crm  \n",
      "2      job_email_blast  \n",
      "3      job_web_traffic  \n",
      "4  job_conversion_rate  \n",
      "\n",
      "--- Value Counts for analysis_results_20250501 ---\n",
      "analysis_results_20250501\n",
      "success    8\n",
      "error      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def update_tracker_with_results(\n",
    "    df: pd.DataFrame,\n",
    "    analysis_results: dict,\n",
    "    date_string: str,\n",
    "    problematic_remote_paths: list | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Updates the DataFrame with log analysis results for the given date.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to update.\n",
    "        analysis_results (dict): Dictionary mapping log filenames to statuses.\n",
    "        date_string (str): The current date string (YYYYMMDD) for the results column.\n",
    "        problematic_remote_paths (list | None): Optional list of remote paths\n",
    "                                                 that had download/access issues.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    results_col = f\"{RESULTS_COLUMN_PREFIX}{date_string}\"\n",
    "    logging.info(f\"Updating tracker DataFrame with results in column: {results_col}\")\n",
    "\n",
    "    # Initialize or reset the results column for the current run\n",
    "    if results_col not in df.columns:\n",
    "        # Insert new column before the last column if possible, otherwise append\n",
    "        try:\n",
    "            # Find a suitable position (e.g., after LOG_PATH_COLUMN or at the end)\n",
    "            insert_pos = df.columns.get_loc(LOG_PATH_COLUMN) + 1\n",
    "        except KeyError:\n",
    "            insert_pos = len(df.columns) # Append if LOG_PATH_COLUMN not found\n",
    "        df.insert(insert_pos, results_col, 'not_analyzed')\n",
    "        logging.info(f\"Added new results column: {results_col}\")\n",
    "    else:\n",
    "        logging.info(f\"Resetting existing results column: {results_col}\")\n",
    "        df[results_col] = 'not_analyzed' # Reset statuses for this run\n",
    "\n",
    "    if problematic_remote_paths is None:\n",
    "        problematic_remote_paths = []\n",
    "\n",
    "    # --- Match results back to the DataFrame ---\n",
    "    analyzed_rows = set() # Keep track of rows updated based on a found log file\n",
    "\n",
    "    for log_filename, status in analysis_results.items():\n",
    "        if status == 'directory_not_found': # Skip if analysis function indicated dir missing\n",
    "            continue\n",
    "\n",
    "        # --- Attempt to match log filename to DataFrame rows ---\n",
    "        # Strategy: Extract job name from filename (part before '-YYYYMMDD')\n",
    "        # and match it against the last component of the remote_log_directory path.\n",
    "        try:\n",
    "            # Handle potential variations in filename format (e.g., no date)\n",
    "            base_job_name = log_filename.split('-')[0] if '-' in log_filename else os.path.splitext(log_filename)[0]\n",
    "        except Exception:\n",
    "            logging.warning(f\"Could not extract base job name from log filename: {log_filename}\")\n",
    "            continue # Skip if filename format is unexpected\n",
    "\n",
    "        matched_indices = []\n",
    "        for index, row in df.iterrows():\n",
    "             remote_path = str(row[LOG_PATH_COLUMN]) if pd.notna(row[LOG_PATH_COLUMN]) else None\n",
    "             if remote_path:\n",
    "                 # Extract expected identifier from remote path (last component)\n",
    "                 path_identifier = os.path.basename(remote_path.rstrip('/'))\n",
    "                 # Check if extracted base job name matches the path identifier\n",
    "                 if base_job_name == path_identifier:\n",
    "                     matched_indices.append(index)\n",
    "\n",
    "        if matched_indices:\n",
    "             # Update all rows that correspond to this log file/job name\n",
    "             for idx in matched_indices:\n",
    "                 df.loc[idx, results_col] = status\n",
    "                 analyzed_rows.add(idx) # Mark this row as having its status set by an analyzed log\n",
    "             logging.info(f\"Matched '{log_filename}' (status: {status}) to {len(matched_indices)} tracker rows (Indices: {matched_indices}).\")\n",
    "        else:\n",
    "             logging.warning(f\"Could not match log file '{log_filename}' to any row in the tracker based on path identifier '{base_job_name}'.\")\n",
    "\n",
    "    # --- Handle rows where logs might not have been downloaded or analyzed ---\n",
    "    logging.info(\"Performing final status updates for rows without direct log matches...\")\n",
    "    for index, row in df.iterrows():\n",
    "        current_status = df.loc[index, results_col]\n",
    "        remote_path = str(row[LOG_PATH_COLUMN]) if pd.notna(row[LOG_PATH_COLUMN]) else None\n",
    "\n",
    "        # 1. Mark rows for paths known to have access/download errors\n",
    "        if remote_path in problematic_remote_paths:\n",
    "            if current_status == 'not_analyzed': # Only overwrite if not already set by analysis\n",
    "                 df.loc[index, results_col] = 'access_error'\n",
    "                 logging.debug(f\"Row {index}: Marked as 'access_error' due to problematic path '{remote_path}'.\")\n",
    "\n",
    "        # 2. Mark rows that were not updated by any analyzed log file\n",
    "        elif index not in analyzed_rows and current_status == 'not_analyzed':\n",
    "            if not remote_path:\n",
    "                df.loc[index, results_col] = 'missing_path'\n",
    "                logging.debug(f\"Row {index}: Marked as 'missing_path'.\")\n",
    "            else:\n",
    "                # Log wasn't downloaded, wasn't found locally, or couldn't be matched\n",
    "                df.loc[index, results_col] = 'log_not_found_or_analyzed'\n",
    "                logging.debug(f\"Row {index}: Marked as 'log_not_found_or_analyzed' for path '{remote_path}'.\")\n",
    "\n",
    "    logging.info(\"Tracker update process finished.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_analysis_results(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"Saves the updated DataFrame back to the Excel tracker file.\"\"\"\n",
    "    logging.info(f\"Attempting to save updated tracker to: {filename}\")\n",
    "    try:\n",
    "        # Use ExcelWriter for potentially better handling if needed, but direct save is fine\n",
    "        df.to_excel(filename, index=False, engine='openpyxl')\n",
    "        logging.info(f\"Analysis results saved successfully.\")\n",
    "    except PermissionError:\n",
    "         logging.error(f\"Permission denied saving analysis results to '{filename}'. Is the file open?\")\n",
    "         print(f\"\\nERROR: Could not save tracker - Permission Denied. Close the file '{filename}' and retry.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save analysis results to '{filename}': {e}\")\n",
    "        print(f\"\\nERROR: Failed to save tracker file: {e}\")\n",
    "\n",
    "\n",
    "# --- Determine Local Log Directory ---\n",
    "\n",
    "today_date_str = get_current_date_string()\n",
    "local_log_dir_to_analyze = get_log_download_directory(LOCAL_LOG_STORAGE_BASE, today_date_str)\n",
    "\n",
    "\n",
    "def analyze_downloaded_logs(local_log_dir: str) -> dict:\n",
    "    \"\"\"(Copied from Notebook 4 for execution context)\"\"\"\n",
    "    log_analysis_results = {}\n",
    "    logging.info(f\"Executing analysis function for directory: {local_log_dir}\")\n",
    "    if not os.path.isdir(local_log_dir):\n",
    "        logging.error(f\"Local log directory not found: {local_log_dir}.\")\n",
    "        return {'error': 'directory_not_found'}\n",
    "    local_log_files = os.listdir(local_log_dir)\n",
    "    if not local_log_files:\n",
    "        logging.warning(f\"No log files found in local directory: {local_log_dir}\")\n",
    "        return {}\n",
    "    for log_filename in local_log_files:\n",
    "        local_log_path = os.path.join(local_log_dir, log_filename)\n",
    "        analysis_status = 'unknown'\n",
    "        try:\n",
    "            with open(local_log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.readlines()\n",
    "                for line in reversed(content):\n",
    "                    if SUCCESS_KEYWORD in line: analysis_status = 'success'; break\n",
    "                    elif FAILURE_KEYWORD in line: analysis_status = 'failure'; break\n",
    "                    elif ERROR_KEYWORD in line: analysis_status = 'error'; break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing {log_filename}: {e}\"); analysis_status = 'parse_error'\n",
    "        log_analysis_results[log_filename] = analysis_status\n",
    "    return log_analysis_results\n",
    "\n",
    "analysis_results_dict = analyze_downloaded_logs(local_log_dir_to_analyze)\n",
    "\n",
    "# Load tracker dataframe\n",
    "analysis_df = get_analysis_tracker(ANALYSIS_TRACKER_PATH, LOG_PATH_COLUMN)\n",
    "\n",
    "# Update and save\n",
    "if analysis_df is not None and analysis_results_dict:\n",
    "    if 'error' in analysis_results_dict and analysis_results_dict['error'] == 'directory_not_found':\n",
    "         print(f\"\\nSkipping update: Local log directory '{local_log_dir_to_analyze}' not found.\")\n",
    "         logging.error(f\"Local log directory '{local_log_dir_to_analyze}' not found. Cannot update tracker.\")\n",
    "    else:\n",
    "        # Simulate problematic paths if needed for testing update logic\n",
    "        problematic_paths_example = [] # Assuming none\n",
    "\n",
    "        # Update the df\n",
    "        updated_df = update_tracker_with_results(\n",
    "            analysis_df.copy(), # Pass a copy to avoid modifying original in memory if needed\n",
    "            analysis_results_dict,\n",
    "            today_date_str,\n",
    "            problematic_paths_example\n",
    "        )\n",
    "\n",
    "        # Save the updated DataFrame\n",
    "        save_analysis_results(updated_df, ANALYSIS_TRACKER_PATH)\n",
    "\n",
    "        print(\"\\n--- Updated Tracker DataFrame Head ---\")\n",
    "        print(updated_df.head())\n",
    "        results_col_name = f\"{RESULTS_COLUMN_PREFIX}{today_date_str}\"\n",
    "        if results_col_name in updated_df.columns:\n",
    "             print(f\"\\n--- Value Counts for {results_col_name} ---\")\n",
    "             print(updated_df[results_col_name].value_counts())\n",
    "\n",
    "elif analysis_df is None:\n",
    "     print(\"\\nSkipping update: Failed to load the tracker DataFrame.\")\n",
    "     logging.error(\"Tracker DataFrame could not be loaded. Update skipped.\")\n",
    "else: # analysis_results_dict is empty or indicates error\n",
    "     print(f\"\\nSkipping update: No analysis results generated from '{local_log_dir_to_analyze}'.\")\n",
    "     logging.warning(f\"No analysis results available. Check directory '{local_log_dir_to_analyze}'. Update skipped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
